{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\cdron\\\\Desktop\\\\Cleanup\\\\Misc\\\\kaggle\\\\Facebook_rec_4'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\cdron\\Desktop\\Cleanup\\Misc\\kaggle\\Facebook_rec_4\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MERCHANDISE_TYPES = ('auto parts', 'books and music', 'clothing', 'computers',\n",
    "                     'furniture', 'home goods', 'jewelry', 'mobile',\n",
    "                     'office equipment', 'sporting goods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COUNTRIES = ('ad', 'ae', 'af', 'ag', 'al', 'am', 'an', 'ao', 'ar', 'at', 'au',\n",
    "             'aw', 'az', 'ba', 'bb', 'bd', 'be', 'bf', 'bg', 'bh', 'bi', 'bj',\n",
    "             'bm', 'bn', 'bo', 'br', 'bs', 'bt', 'bw', 'by', 'bz', 'ca', 'cd',\n",
    "             'cf', 'cg', 'ch', 'ci', 'cl', 'cm', 'cn', 'co', 'cr', 'cv', 'cy',\n",
    "             'cz', 'de', 'dj', 'dk', 'dm', 'do', 'dz', 'ec', 'ee', 'eg', 'er',\n",
    "             'es', 'et', 'eu', 'fi', 'fj', 'fo', 'fr', 'ga', 'gb', 'ge', 'gh',\n",
    "             'gi', 'gl', 'gm', 'gn', 'gp', 'gq', 'gr', 'gt', 'gu', 'gy', 'hk',\n",
    "             'hn', 'hr', 'ht', 'hu', 'id', 'ie', 'il', 'in', 'iq', 'ir', 'is',\n",
    "             'it', 'je', 'jm', 'jo', 'jp', 'ke', 'kg', 'kh', 'kr', 'kw', 'kz',\n",
    "             'la', 'lb', 'li', 'lk', 'lr', 'ls', 'lt', 'lu', 'lv', 'ly', 'ma',\n",
    "             'mc', 'md', 'me', 'mg', 'mh', 'mk', 'ml', 'mm', 'mn', 'mo', 'mp',\n",
    "             'mr', 'mt', 'mu', 'mv', 'mw', 'mx', 'my', 'mz', 'na', 'nc', 'ne',\n",
    "             'ng', 'ni', 'nl', 'no', 'np', 'nz', 'om', 'pa', 'pe', 'pf', 'pg',\n",
    "             'ph', 'pk', 'pl', 'pr', 'ps', 'pt', 'py', 'qa', 're', 'ro', 'rs',\n",
    "             'ru', 'rw', 'sa', 'sb', 'sc', 'sd', 'se', 'sg', 'si', 'sk', 'sl',\n",
    "             'sn', 'so', 'sr', 'sv', 'sy', 'sz', 'tc', 'td', 'tg', 'th', 'tj',\n",
    "             'tl', 'tm', 'tn', 'tr', 'tt', 'tw', 'tz', 'ua', 'ug', 'uk', 'us',\n",
    "             'uy', 'uz', 'vc', 've', 'vi', 'vn', 'ws', 'ye', 'za', 'zm', 'zw',\n",
    "             'zz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature_extraction():\n",
    "    bidder_dict = defaultdict(int)\n",
    "    bid_auc_dict = defaultdict(dict)\n",
    "    bid_merch_dict = defaultdict(dict)\n",
    "    bid_device_dict = defaultdict(set)\n",
    "    bid_country_dict = defaultdict(dict)\n",
    "    bid_ip_dict = defaultdict(set)\n",
    "    bid_url_dict = defaultdict(set)\n",
    "    with open('bids.csv', 'rt') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile, delimiter=',')\n",
    "        labels = next(csv_reader)\n",
    "        for idx, row_ in enumerate(csv_reader):\n",
    "            if idx % 100000 == 0:\n",
    "                print ('processed %d' % idx)\n",
    "            row = dict(zip(labels, row_))\n",
    "            bid = row['bidder_id']\n",
    "            bidder_dict[bid] += 1\n",
    "            if row['auction'] not in bid_auc_dict[bid]:\n",
    "                bid_auc_dict[bid][row['auction']] = 0\n",
    "            bid_auc_dict[bid][row['auction']] += 1\n",
    "            if row['merchandise'] not in bid_merch_dict[bid]:\n",
    "                bid_merch_dict[bid][row['merchandise']] = 0\n",
    "            bid_merch_dict[bid][row['merchandise']] += 1\n",
    "            bid_device_dict[bid].add(row['device'])\n",
    "            if row['country'] not in bid_country_dict[bid]:\n",
    "                bid_country_dict[bid][row['country']] = 0\n",
    "            bid_country_dict[bid][row['country']] += 1\n",
    "            bid_ip_dict[bid].add(row['ip'])\n",
    "            bid_url_dict[bid].add(row['url'])\n",
    "    labels_to_write = ['bidder_id', 'n_auctions', 'n_per_auc', 'n_devices', \n",
    "                       'country', 'n_countries', 'n_ip', 'n_url']\n",
    "    for merch in MERCHANDISE_TYPES:\n",
    "        labels_to_write.append(merch)\n",
    "    with open('bid_reduced.csv', 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(labels_to_write)\n",
    "        for bidder in sorted(bidder_dict.keys()):\n",
    "            row_dict = {k: None for k in labels_to_write}\n",
    "            row_dict['bidder_id'] = bidder\n",
    "            row_dict['n_auctions'] = len(bid_auc_dict[bidder])\n",
    "            row_dict['n_per_auc'] = sum(bid_auc_dict[bidder].values())\\\n",
    "                                    / float(len(bid_auc_dict[bidder]))\n",
    "            row_dict['n_devices'] = len(bid_device_dict[bidder])\n",
    "            row_dict['country'] = sorted(bid_country_dict[bidder].items(), \n",
    "                                         key=lambda x: x[1],\n",
    "                                         reverse=True)[0][0]\n",
    "            row_dict['n_countries'] = len(bid_country_dict[bidder])\n",
    "            row_dict['n_ip'] = len(bid_ip_dict[bidder])\n",
    "            row_dict['n_url'] = len(bid_url_dict[bidder])\n",
    "            for merch in MERCHANDISE_TYPES:\n",
    "                if merch in bid_merch_dict[bidder]:\n",
    "                    row_dict[merch] = bid_merch_dict[bidder][merch]\n",
    "                else:\n",
    "                    row_dict[merch] = 0\n",
    "            row_val = [row_dict[col] for col in labels_to_write]\n",
    "            csv_writer.writerow(row_val)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0\n",
      "processed 100000\n",
      "processed 200000\n",
      "processed 300000\n",
      "processed 400000\n",
      "processed 500000\n",
      "processed 600000\n",
      "processed 700000\n",
      "processed 800000\n",
      "processed 900000\n",
      "processed 1000000\n",
      "processed 1100000\n",
      "processed 1200000\n",
      "processed 1300000\n",
      "processed 1400000\n",
      "processed 1500000\n",
      "processed 1600000\n",
      "processed 1700000\n",
      "processed 1800000\n",
      "processed 1900000\n",
      "processed 2000000\n",
      "processed 2100000\n",
      "processed 2200000\n",
      "processed 2300000\n",
      "processed 2400000\n",
      "processed 2500000\n",
      "processed 2600000\n",
      "processed 2700000\n",
      "processed 2800000\n",
      "processed 2900000\n",
      "processed 3000000\n",
      "processed 3100000\n",
      "processed 3200000\n",
      "processed 3300000\n",
      "processed 3400000\n",
      "processed 3500000\n",
      "processed 3600000\n",
      "processed 3700000\n",
      "processed 3800000\n",
      "processed 3900000\n",
      "processed 4000000\n",
      "processed 4100000\n",
      "processed 4200000\n",
      "processed 4300000\n",
      "processed 4400000\n",
      "processed 4500000\n",
      "processed 4600000\n",
      "processed 4700000\n",
      "processed 4800000\n",
      "processed 4900000\n",
      "processed 5000000\n",
      "processed 5100000\n",
      "processed 5200000\n",
      "processed 5300000\n",
      "processed 5400000\n",
      "processed 5500000\n",
      "processed 5600000\n",
      "processed 5700000\n",
      "processed 5800000\n",
      "processed 5900000\n",
      "processed 6000000\n",
      "processed 6100000\n",
      "processed 6200000\n",
      "processed 6300000\n",
      "processed 6400000\n",
      "processed 6500000\n",
      "processed 6600000\n",
      "processed 6700000\n",
      "processed 6800000\n",
      "processed 6900000\n",
      "processed 7000000\n",
      "processed 7100000\n",
      "processed 7200000\n",
      "processed 7300000\n",
      "processed 7400000\n",
      "processed 7500000\n",
      "processed 7600000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    feature_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(indf):\n",
    "    for col in ['outcome', 'n_auctions', 'n_devices', 'n_countries', 'n_ip', \n",
    "                'n_url'] + list(MERCHANDISE_TYPES):\n",
    "        if col in indf.columns:\n",
    "            indf[col] = indf[col].astype(int)\n",
    "    \n",
    "    indf['country'] = indf['country'].map({k: i for (i, k)\n",
    "                                          in enumerate(COUNTRIES)})\n",
    "    indf.loc[:, 'country'][indf['country'].isnull()] = -1\n",
    "    indf['country'] = indf['country'].astype(int)\n",
    "\n",
    "    indf = indf.drop(labels=['payment_account', 'address'], \n",
    "                     axis=1)\n",
    "    return indf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(do_plots=False):\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "    submit_df = pd.read_csv('sampleSubmission.csv')\n",
    "    bid_df = pd.read_csv('bid_reduced.csv')\n",
    "\n",
    "    train_df = train_df.merge(bid_df, on='bidder_id', how='inner')\n",
    "    test_df = test_df.merge(bid_df, on='bidder_id', how='inner')\n",
    "    \n",
    "    train_df = clean_data(train_df)\n",
    "    test_df = clean_data(test_df)\n",
    "    \n",
    "    if do_plots:\n",
    "        from plot_data import plot_data\n",
    "        plot_data(train_df, prefix='html_train')\n",
    "        plot_data(test_df, prefix='html_test')\n",
    "\n",
    "    print (train_df.dtypes)\n",
    "    print (test_df.dtypes)\n",
    "    print (submit_df.dtypes)\n",
    "\n",
    "    xtrain = train_df.drop(labels=['outcome','bidder_id'], axis=1).values\n",
    "    ytrain = train_df['outcome'].values\n",
    "    xtest = test_df.drop(labels=['bidder_id'], axis=1).values\n",
    "    ytest = submit_df\n",
    "    y_id = list(test_df['bidder_id'])\n",
    "\n",
    "    return xtrain, ytrain, xtest, ytest, y_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bidder_id            object\n",
      "outcome               int32\n",
      "n_auctions            int32\n",
      "n_per_auc           float64\n",
      "n_devices             int32\n",
      "country               int32\n",
      "n_countries           int32\n",
      "n_ip                  int32\n",
      "n_url                 int32\n",
      "auto parts            int32\n",
      "books and music       int32\n",
      "clothing              int32\n",
      "computers             int32\n",
      "furniture             int32\n",
      "home goods            int32\n",
      "jewelry               int32\n",
      "mobile                int32\n",
      "office equipment      int32\n",
      "sporting goods        int32\n",
      "dtype: object\n",
      "bidder_id            object\n",
      "n_auctions            int32\n",
      "n_per_auc           float64\n",
      "n_devices             int32\n",
      "country               int32\n",
      "n_countries           int32\n",
      "n_ip                  int32\n",
      "n_url                 int32\n",
      "auto parts            int32\n",
      "books and music       int32\n",
      "clothing              int32\n",
      "computers             int32\n",
      "furniture             int32\n",
      "home goods            int32\n",
      "jewelry               int32\n",
      "mobile                int32\n",
      "office equipment      int32\n",
      "sporting goods        int32\n",
      "dtype: object\n",
      "bidder_id      object\n",
      "prediction    float64\n",
      "dtype: object\n",
      "[(1984, 17), (1984,), (4630, 17), (4700, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3 2.1.0\\lib\\site-packages\\IPython\\kernel\\__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    xtrain, ytrain, xtest, ytest, y_id = load_data(do_plots=False)\n",
    "\n",
    "    print ([x.shape for x in (xtrain, ytrain, xtest, ytest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_model(model, xtrain, ytrain):\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(xtrain, ytrain,\n",
    "                                                    test_size=0.2)\n",
    "    model.fit(xTrain, yTrain)\n",
    "    \n",
    "    \n",
    "    yprob = model.predict_proba(xTest)\n",
    "    \n",
    "    print ('score', roc_auc_score(yTest, yprob[:,1]))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_submission(model, xtest, ytest, yid):\n",
    "    yprob = model.predict_proba(xtest)\n",
    "    print (yprob.shape, ytest.shape)\n",
    "    for idx, row in ytest.iterrows():\n",
    "        if row['bidder_id'] in yid:\n",
    "            idy = yid.index(row['bidder_id'])\n",
    "            ytest.loc[idx, 'prediction'] = yprob[idy, 1]\n",
    "        else:\n",
    "            ytest.loc[idx, 'prediction'] = 0.05 # pure hack\n",
    "    print (ytest.shape)\n",
    "    ytest.to_csv('may10th_submission_2_2.csv', index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bidder_id            object\n",
      "outcome               int32\n",
      "n_auctions            int32\n",
      "n_per_auc           float64\n",
      "n_devices             int32\n",
      "country               int32\n",
      "n_countries           int32\n",
      "n_ip                  int32\n",
      "n_url                 int32\n",
      "auto parts            int32\n",
      "books and music       int32\n",
      "clothing              int32\n",
      "computers             int32\n",
      "furniture             int32\n",
      "home goods            int32\n",
      "jewelry               int32\n",
      "mobile                int32\n",
      "office equipment      int32\n",
      "sporting goods        int32\n",
      "dtype: object\n",
      "bidder_id            object\n",
      "n_auctions            int32\n",
      "n_per_auc           float64\n",
      "n_devices             int32\n",
      "country               int32\n",
      "n_countries           int32\n",
      "n_ip                  int32\n",
      "n_url                 int32\n",
      "auto parts            int32\n",
      "books and music       int32\n",
      "clothing              int32\n",
      "computers             int32\n",
      "furniture             int32\n",
      "home goods            int32\n",
      "jewelry               int32\n",
      "mobile                int32\n",
      "office equipment      int32\n",
      "sporting goods        int32\n",
      "dtype: object\n",
      "bidder_id      object\n",
      "prediction    float64\n",
      "dtype: object\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2474            2.99s\n",
      "         2           0.2034            2.77s\n",
      "         3           0.1739            2.76s\n",
      "         4           0.1528            2.79s\n",
      "         5           0.1363            3.00s\n",
      "         6           0.1227            2.94s\n",
      "         7           0.1106            2.95s\n",
      "         8           0.1003            3.12s\n",
      "         9           0.0911            3.14s\n",
      "        10           0.0836            3.06s\n",
      "        20           0.0371            3.29s\n",
      "        30           0.0188            3.23s\n",
      "        40           0.0108            3.00s\n",
      "        50           0.0078            2.86s\n",
      "        60           0.0066            2.65s\n",
      "        70           0.0061            2.43s\n",
      "        80           0.0059            2.18s\n",
      "        90           0.0058            1.93s\n",
      "       100           0.0057            1.65s\n",
      "       200           0.0056            0.00s\n",
      "score 0.855836236934\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2973            3.78s\n",
      "         2           0.2585            3.47s\n",
      "         3           0.2336            3.35s\n",
      "         4           0.2113            3.77s\n",
      "         5           0.1839            4.06s\n",
      "         6           0.1658            4.43s\n",
      "         7           0.1512            4.58s\n",
      "         8           0.1395            4.75s\n",
      "         9           0.1289            4.84s\n",
      "        10           0.1196            4.92s\n",
      "        20           0.0670            4.80s\n",
      "        30           0.0360            4.37s\n",
      "        40           0.0243            4.17s\n",
      "        50           0.0168            4.00s\n",
      "        60           0.0133            3.62s\n",
      "        70           0.0114            3.29s\n",
      "        80           0.0104            2.96s\n",
      "        90           0.0097            2.64s\n",
      "       100           0.0093            2.32s\n",
      "       200           0.0085            0.00s\n",
      "(4630, 2) (4700, 2)\n",
      "(4700, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3 2.1.0\\lib\\site-packages\\IPython\\kernel\\__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    xtrain, ytrain, xtest, ytest, yid = load_data(do_plots=False)\n",
    "\n",
    "#    model = LogisticRegression()\n",
    "#    model = RandomForestClassifier()\n",
    "    model = GradientBoostingClassifier(n_estimators=200, learning_rate=0.095, min_samples_split=3, max_depth=9, verbose=1)\n",
    "#  model = svm.NuSVC(probability=True)\n",
    "    test_model(model, xtrain, ytrain)\n",
    "\n",
    "    model.fit(xtrain, ytrain)\n",
    "    \n",
    "    prepare_submission(model, xtest, ytest, yid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(init=None, learning_rate=0.095, loss='deviance',\n",
       "              max_depth=9, max_features=None, max_leaf_nodes=None,\n",
       "              min_samples_leaf=1, min_samples_split=3,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "              random_state=None, subsample=1.0, verbose=1,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
